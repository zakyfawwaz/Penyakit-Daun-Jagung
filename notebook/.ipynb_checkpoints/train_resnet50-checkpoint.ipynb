{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training ResNet-50 untuk Deteksi Penyakit Daun Jagung\n",
        "\n",
        "Notebook ini berisi script lengkap untuk training model ResNet-50 dengan 3 kelas:\n",
        "- **Hawar**: Daun jagung terkena penyakit hawar\n",
        "- **Sehat**: Daun jagung dalam kondisi sehat\n",
        "- **Karat**: Daun jagung terkena penyakit karat\n",
        "\n",
        "**Target**: Training selama 50 epoch dengan evaluasi lengkap\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A. Import Libraries dan Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Menggunakan device: {device}')\n",
        "\n",
        "# Set random seed untuk reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. Persiapan Dataset\n",
        "\n",
        "Struktur dataset yang diharapkan:\n",
        "```\n",
        "dataset/\n",
        "    train/\n",
        "        hawar/\n",
        "        sehat/\n",
        "        karat/\n",
        "    val/\n",
        "        hawar/\n",
        "        sehat/\n",
        "        karat/\n",
        "```\n",
        "\n",
        "Jika dataset masih dalam format folder tunggal, kita akan membuat script untuk memisahkannya.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path dataset\n",
        "dataset_root = 'dataset'  # Path ke folder dataset utama (sekarang di notebook/dataset)\n",
        "train_dir = 'dataset/train'\n",
        "val_dir = 'dataset/val'\n",
        "\n",
        "# Buat struktur folder jika belum ada\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Kelas yang akan digunakan\n",
        "classes = ['hawar', 'sehat', 'karat']\n",
        "\n",
        "# Buat folder untuk setiap kelas di train dan val\n",
        "for split in ['train', 'val']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(f'dataset/{split}/{cls}', exist_ok=True)\n",
        "\n",
        "print(\"Struktur folder dataset sudah dibuat!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk memisahkan dataset menjadi train dan validation\n",
        "def split_dataset(source_dir, train_dir, val_dir, split_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Memisahkan dataset dari folder sumber ke train dan validation\n",
        "    source_dir: folder utama yang berisi subfolder Hawar, Sehat, Karat\n",
        "    \"\"\"\n",
        "    # Mapping nama folder\n",
        "    class_mapping = {\n",
        "        'Hawar': 'hawar',\n",
        "        'Sehat': 'sehat',\n",
        "        'Karat': 'karat'\n",
        "    }\n",
        "    \n",
        "    for class_folder in os.listdir(source_dir):\n",
        "        if class_folder in class_mapping:\n",
        "            source_path = os.path.join(source_dir, class_folder)\n",
        "            if os.path.isdir(source_path):\n",
        "                # Ambil semua file gambar\n",
        "                images = [f for f in os.listdir(source_path) \n",
        "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "                \n",
        "                # Shuffle\n",
        "                np.random.shuffle(images)\n",
        "                \n",
        "                # Split\n",
        "                split_idx = int(len(images) * split_ratio)\n",
        "                train_images = images[:split_idx]\n",
        "                val_images = images[split_idx:]\n",
        "                \n",
        "                # Copy ke folder train\n",
        "                target_class = class_mapping[class_folder]\n",
        "                for img in train_images:\n",
        "                    src = os.path.join(source_path, img)\n",
        "                    dst = os.path.join(train_dir, target_class, img)\n",
        "                    shutil.copy2(src, dst)\n",
        "                \n",
        "                # Copy ke folder val\n",
        "                for img in val_images:\n",
        "                    src = os.path.join(source_path, img)\n",
        "                    dst = os.path.join(val_dir, target_class, img)\n",
        "                    shutil.copy2(src, dst)\n",
        "                \n",
        "                print(f'{class_folder}: {len(train_images)} train, {len(val_images)} val')\n",
        "\n",
        "# Jalankan split dataset untuk memisahkan dataset menjadi train dan validation\n",
        "split_dataset(dataset_root, train_dir, val_dir, split_ratio=0.8)\n",
        "print(\"Dataset sudah siap digunakan!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Preprocessing dan Data Augmentation\n",
        "\n",
        "Menggunakan transformasi standar ImageNet untuk preprocessing:\n",
        "- Resize ke 224x224\n",
        "- Normalisasi dengan mean dan std ImageNet\n",
        "- Augmentasi untuk training: RandomFlip, RandomRotation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImageNet normalization parameters\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Transformasi untuk training (dengan augmentasi)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Transformasi untuk validation (tanpa augmentasi)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "print(\"Transformasi sudah didefinisikan!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset menggunakan ImageFolder\n",
        "train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = ImageFolder(root=val_dir, transform=val_transform)\n",
        "\n",
        "# Buat DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'Jumlah kelas: {len(train_dataset.classes)}')\n",
        "print(f'Kelas: {train_dataset.classes}')\n",
        "print(f'Jumlah training samples: {len(train_dataset)}')\n",
        "print(f'Jumlah validation samples: {len(val_dataset)}')\n",
        "print(f'Batch size: {batch_size}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Load Model ResNet-50\n",
        "\n",
        "Menggunakan ResNet-50 pretrained dan mengganti fully connected layer untuk 3 kelas output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ResNet-50 pretrained\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze semua layer kecuali fully connected\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Ganti fully connected layer untuk 3 kelas\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 3)\n",
        "\n",
        "# Unfreeze fully connected layer\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Pindahkan model ke device\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model ResNet-50 sudah dimuat!\")\n",
        "print(f\"Jumlah parameter: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. Setup Training\n",
        "\n",
        "- Optimizer: Adam dengan learning rate 0.001\n",
        "- Loss Function: CrossEntropyLoss\n",
        "- Learning Rate Scheduler: StepLR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "\n",
        "print(\"Optimizer dan loss function sudah disetup!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. Training Loop (50 Epochs)\n",
        "\n",
        "Training model selama 50 epoch dengan tracking loss dan accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk training\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(train_loader, desc='Training'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Fungsi untuk validation\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc='Validation'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print(\"Fungsi training dan validation sudah didefinisikan!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training selama 50 epoch\n",
        "num_epochs = 50\n",
        "\n",
        "# List untuk menyimpan history\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"MEMULAI TRAINING - 50 EPOCH\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nEpoch [{epoch+1}/{num_epochs}]')\n",
        "    print('-' * 50)\n",
        "    \n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict().copy()\n",
        "    \n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    print(f'Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TRAINING SELESAI!\")\n",
        "print(\"=\" * 50)\n",
        "print(f'Best Validation Accuracy: {best_val_acc:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## G. Simpan Model\n",
        "\n",
        "Menyimpan model terbaik ke file `model_resnet50.pth`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model state\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "# Simpan model\n",
        "model_save_path = '../model/model_resnet50.pth'\n",
        "os.makedirs('../model', exist_ok=True)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f'Model berhasil disimpan ke: {model_save_path}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## H. Visualisasi Training History\n",
        "\n",
        "Menampilkan grafik loss dan accuracy untuk training dan validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot Loss\n",
        "axes[0].plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
        "axes[0].plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training dan Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Accuracy\n",
        "axes[1].plot(train_accs, label='Train Accuracy', color='blue', linewidth=2)\n",
        "axes[1].plot(val_accs, label='Validation Accuracy', color='red', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training dan Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nFinal Training Accuracy: {train_accs[-1]:.2f}%')\n",
        "print(f'Final Validation Accuracy: {val_accs[-1]:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I. Evaluasi dengan Confusion Matrix\n",
        "\n",
        "Menampilkan confusion matrix dan classification report untuk evaluasi detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluasi model dengan confusion matrix\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_loader, desc='Evaluating'):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        \n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            cbar_kws={'label': 'Jumlah'})\n",
        "plt.title('Confusion Matrix - Validation Set', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 50)\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## J. Contoh Prediksi Satu Gambar Test\n",
        "\n",
        "Menguji model dengan satu gambar dari validation set dan menampilkan prediksi dengan confidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ambil satu gambar dari validation set\n",
        "model.eval()\n",
        "data_iter = iter(val_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Pilih gambar pertama\n",
        "img = images[0:1].to(device)\n",
        "true_label = labels[0].item()\n",
        "\n",
        "# Prediksi\n",
        "with torch.no_grad():\n",
        "    outputs = model(img)\n",
        "    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "    confidence, predicted = torch.max(probabilities, 0)\n",
        "\n",
        "predicted_class = class_names[predicted.item()]\n",
        "true_class = class_names[true_label]\n",
        "confidence_score = confidence.item() * 100\n",
        "\n",
        "# Denormalize untuk visualisasi\n",
        "def denormalize(tensor, mean, std):\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "img_vis = images[0].clone()\n",
        "img_vis = denormalize(img_vis, mean, std)\n",
        "img_vis = torch.clamp(img_vis, 0, 1)\n",
        "\n",
        "# Plot hasil\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_vis.permute(1, 2, 0))\n",
        "plt.title(f'True Label: {true_class}', fontsize=14, fontweight='bold')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "probs = probabilities.cpu().numpy()\n",
        "colors = ['red' if i == predicted.item() else 'gray' for i in range(len(class_names))]\n",
        "plt.barh(class_names, probs, color=colors)\n",
        "plt.xlabel('Probability', fontsize=12)\n",
        "plt.title(f'Predicted: {predicted_class}\\nConfidence: {confidence_score:.2f}%', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTrue Label: {true_class}\")\n",
        "print(f\"Predicted Label: {predicted_class}\")\n",
        "print(f\"Confidence: {confidence_score:.2f}%\")\n",
        "print(f\"Correct: {'✓' if predicted_class == true_class else '✗'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K. Unit Test - Load Model dan Prediksi\n",
        "\n",
        "Unit test untuk memastikan model dapat dimuat dan melakukan prediksi dengan benar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unit Test: Load model dan prediksi\n",
        "print(\"=\" * 50)\n",
        "print(\"UNIT TEST - Load Model dan Prediksi\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load model\n",
        "test_model = models.resnet50(pretrained=False)\n",
        "num_features = test_model.fc.in_features\n",
        "test_model.fc = nn.Linear(num_features, 3)\n",
        "test_model.load_state_dict(torch.load('../model/model_resnet50.pth', map_location=device))\n",
        "test_model = test_model.to(device)\n",
        "test_model.eval()\n",
        "\n",
        "print(\"✓ Model berhasil dimuat\")\n",
        "\n",
        "# Test dengan satu gambar\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Ambil satu gambar dari validation set\n",
        "test_img_path = None\n",
        "for root, dirs, files in os.walk(val_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            test_img_path = os.path.join(root, file)\n",
        "            break\n",
        "    if test_img_path:\n",
        "        break\n",
        "\n",
        "if test_img_path:\n",
        "    # Load dan preprocess gambar\n",
        "    img = Image.open(test_img_path).convert('RGB')\n",
        "    img_tensor = test_transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model(img_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "        confidence, predicted = torch.max(probabilities, 0)\n",
        "    \n",
        "    predicted_class = class_names[predicted.item()]\n",
        "    confidence_score = confidence.item() * 100\n",
        "    \n",
        "    print(f\"✓ Gambar test: {os.path.basename(test_img_path)}\")\n",
        "    print(f\"✓ Prediksi: {predicted_class}\")\n",
        "    print(f\"✓ Confidence: {confidence_score:.2f}%\")\n",
        "    print(f\"✓ Probabilitas per kelas:\")\n",
        "    for i, cls in enumerate(class_names):\n",
        "        print(f\"    {cls}: {probabilities[i].item()*100:.2f}%\")\n",
        "else:\n",
        "    print(\"✗ Tidak ada gambar test yang ditemukan\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"UNIT TEST SELESAI\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L. Kesimpulan\n",
        "\n",
        "Training model ResNet-50 untuk deteksi penyakit daun jagung telah selesai dengan hasil:\n",
        "- **Total Epoch**: 50\n",
        "- **Best Validation Accuracy**: {best_val_acc:.2f}%\n",
        "- **Model tersimpan di**: `../model/model_resnet50.pth`\n",
        "\n",
        "Model siap digunakan untuk inference di aplikasi Flask!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
